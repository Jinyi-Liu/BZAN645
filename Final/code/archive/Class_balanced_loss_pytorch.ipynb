{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvqQjmvj00qO"
      },
      "source": [
        "# Class-balanced-loss-pytorch\n",
        "\n",
        "Pytorch implementation on [Github](https://github.com/vandit15/Class-balanced-loss-pytorch/tree/master) of the paper\n",
        "[Class-Balanced Loss Based on Effective Number of Samples](https://arxiv.org/abs/1901.05555) presented at CVPR'19.\n",
        "\n",
        "See also: [Medium Article](https://medium.com/@vandit_15/handling-class-imbalanced-data-using-a-loss-specifically-made-for-it-6e58fd65ffab?source=friends_link&sk=ac09ea6061990ead2a2f90e3767ae91f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDBopY_21uPF"
      },
      "source": [
        "## Key Idea:\n",
        "\n",
        "* To address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented).\n",
        "* As the number of samples increases, the additional benefit\n",
        "of a newly added data point will diminish.\n",
        "* Measure data overlap by associating with each sample a small neighboring region rather than a single point.\n",
        "* The effective number of samples is calculated as $(1−β^n)/(1−β)$,\n",
        "where $n$ is the number of samples and $β \\in [0, 1)$ is a hyperparameter.\n",
        "* Design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding\n",
        "a class-balanced loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wq1fXQNk00EV"
      },
      "outputs": [],
      "source": [
        "\"\"\"Pytorch implementation of Class-Balanced-Loss\n",
        "   Reference: \"Class-Balanced Loss Based on Effective Number of Samples\"\n",
        "   Authors: Yin Cui and\n",
        "               Menglin Jia and\n",
        "               Tsung Yi Lin and\n",
        "               Yang Song and\n",
        "               Serge J. Belongie\n",
        "   https://arxiv.org/abs/1901.05555, CVPR'19.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def focal_loss(labels, logits, alpha, gamma):\n",
        "    \"\"\"Compute the focal loss between `logits` and the ground truth `labels`.\n",
        "\n",
        "    Focal loss = -alpha_t * (1-pt)^gamma * log(pt)\n",
        "    where pt is the probability of being classified to the true class.\n",
        "    pt = p (if true class), otherwise pt = 1 - p. p = sigmoid(logit).\n",
        "\n",
        "    Args:\n",
        "      labels: A float tensor of size [batch, num_classes].\n",
        "      logits: A float tensor of size [batch, num_classes].\n",
        "      alpha: A float tensor of size [batch_size]\n",
        "        specifying per-example weight for balanced cross entropy.\n",
        "      gamma: A float scalar modulating loss from hard and easy examples.\n",
        "\n",
        "    Returns:\n",
        "      focal_loss: A float32 scalar representing normalized total loss.\n",
        "    \"\"\"\n",
        "    BCLoss = F.binary_cross_entropy_with_logits(input = logits, target = labels,reduction = \"none\")\n",
        "\n",
        "    if gamma == 0.0:\n",
        "        modulator = 1.0\n",
        "    else:\n",
        "        modulator = torch.exp(-gamma * labels * logits - gamma * torch.log(1 +\n",
        "            torch.exp(-1.0 * logits)))\n",
        "\n",
        "    loss = modulator * BCLoss\n",
        "\n",
        "    weighted_loss = alpha * loss\n",
        "    focal_loss = torch.sum(weighted_loss)\n",
        "\n",
        "    focal_loss /= torch.sum(labels)\n",
        "    return focal_loss\n",
        "\n",
        "def CB_loss(labels, logits, samples_per_cls, no_of_classes, loss_type, beta, gamma):\n",
        "    \"\"\"Compute the Class Balanced Loss between `logits` and the ground truth `labels`.\n",
        "\n",
        "    Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)\n",
        "    where Loss is one of the standard losses used for Neural Networks.\n",
        "\n",
        "    Args:\n",
        "      labels: A int tensor of size [batch].\n",
        "      logits: A float tensor of size [batch, no_of_classes].\n",
        "      samples_per_cls: A python list of size [no_of_classes].\n",
        "      no_of_classes: total number of classes. int\n",
        "      loss_type: string. One of \"sigmoid\", \"focal\", \"softmax\".\n",
        "      beta: float. Hyperparameter for Class balanced loss.\n",
        "      gamma: float. Hyperparameter for Focal loss.\n",
        "\n",
        "    Returns:\n",
        "      cb_loss: A float tensor representing class balanced loss\n",
        "    \"\"\"\n",
        "    effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
        "    weights = (1.0 - beta) / np.array(effective_num)\n",
        "    weights = weights / np.sum(weights) * no_of_classes\n",
        "\n",
        "    labels_one_hot = F.one_hot(labels, no_of_classes).float()\n",
        "\n",
        "    weights = torch.tensor(weights).float()\n",
        "    weights = weights.unsqueeze(0)\n",
        "    weights = weights.repeat(labels_one_hot.shape[0],1) * labels_one_hot\n",
        "    weights = weights.sum(1)\n",
        "    weights = weights.unsqueeze(1)\n",
        "    weights = weights.repeat(1,no_of_classes)\n",
        "\n",
        "    if loss_type == \"focal\":\n",
        "        cb_loss = focal_loss(labels_one_hot, logits, weights, gamma)\n",
        "    elif loss_type == \"sigmoid\":\n",
        "        cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot, weights = weights)\n",
        "    elif loss_type == \"softmax\":\n",
        "        pred = logits.softmax(dim = 1)\n",
        "        cb_loss = F.binary_cross_entropy(input = pred, target = labels_one_hot, weight = weights)\n",
        "    return cb_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq9jYOuiD5YZ",
        "outputId": "73175917-4e15-47d6-b33e-8281cd0f7837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8500)\n"
          ]
        }
      ],
      "source": [
        "no_of_classes = 5\n",
        "logits = torch.rand(10, no_of_classes).float()\n",
        "labels = torch.randint(0, no_of_classes, size = (10,))\n",
        "beta = 0.9999\n",
        "gamma = 2.0\n",
        "samples_per_cls = [5,3,1,2,2]\n",
        "loss_type = \"focal\"\n",
        "cb_loss = CB_loss(labels, logits, samples_per_cls, no_of_classes, loss_type, beta, gamma)\n",
        "print(cb_loss)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
