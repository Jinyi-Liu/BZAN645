{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functions import *\n",
    "path = \"/app/Final/code\"\n",
    "# path = \".\"\n",
    "# This is the dataset processed from the midterm\n",
    "train_size = 14993\n",
    "data_df = pd.read_csv(path + \"/data/data_df_proc.csv\")[:train_size]\n",
    "data_df.head()\n",
    "\n",
    "cols_to_drop = [\"Name\", \"RescuerID\", \"VideoAmt\", \"Description\", \"PetID\", \"PhotoAmt\"]\n",
    "to_drop_columns = [\n",
    "    \"PetID\",\n",
    "    \"Name\",\n",
    "    \"RescuerID\",\n",
    "    \"Description\",\n",
    "    \"BreedName_full\",\n",
    "    \"Breed1Name\",\n",
    "    \"Breed2Name\",\n",
    "]\n",
    "data_df.drop(cols_to_drop + to_drop_columns, axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "# This is necessary only for the neural network\n",
    "# data_df.fillna(data_df.mean(), inplace=True)\n",
    "\n",
    "# Embedding the categorical variables using nn.Embedding\n",
    "cat_cols = [\n",
    "    \"Breed1\",\n",
    "    \"Breed2\",\n",
    "    \"Color1\",\n",
    "    \"Color2\",\n",
    "    \"Color3\",\n",
    "    \"Gender\",\n",
    "    \"State\",\n",
    "    \"Breed_full\",\n",
    "    \"Color_full\",\n",
    "    \"hard_interaction\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "for cat_col in cat_cols:\n",
    "    label_encoders[cat_col] = LabelEncoder()\n",
    "    data_df[cat_col] = label_encoders[cat_col].fit_transform(data_df[cat_col])\n",
    "\n",
    "emb_c = {n: len(col.unique()) for n, col in data_df.items() if n in cat_cols}\n",
    "emb_cols = emb_c.keys()  # names of columns chosen for embedding\n",
    "emb_szs = [\n",
    "    (c, min(5, (c + 1) // 2)) for _, c in emb_c.items()\n",
    "]  # embedding sizes for the chosen columns\n",
    "\n",
    "# Split data into train and validation by AdoptionSpeed and stratify\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_cont = len(data_df.columns) - len(emb_cols) - 1  # number of continuous columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PetFinderModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(176, 5)\n",
       "    (1): Embedding(135, 5)\n",
       "    (2): Embedding(3, 2)\n",
       "    (3-4): 2 x Embedding(7, 4)\n",
       "    (5): Embedding(6, 3)\n",
       "    (6): Embedding(14, 5)\n",
       "    (7): Embedding(812, 5)\n",
       "    (8): Embedding(63, 5)\n",
       "    (9): Embedding(142, 5)\n",
       "  )\n",
       "  (lin1): Linear(in_features=201, out_features=512, bias=True)\n",
       "  (lin2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (lin3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lin4): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (lin5): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (bn1): ReLU()\n",
       "  (bn2): ReLU()\n",
       "  (bn3): ReLU()\n",
       "  (bn4): ReLU()\n",
       "  (output): ReLU()\n",
       "  (emb_drop): Dropout(p=0.2, inplace=False)\n",
       "  (drops): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from network_setting import *\n",
    "model = PetFinderModel(emb_szs, n_cont)\n",
    "model.load_state_dict(torch.load(path + \"/model-stratify.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pet_xgboost(params, X_train, xgb_features, split_index, print_result=True):\n",
    "    oof_train_xgb = np.zeros((X_train.shape[0]))\n",
    "    qwks = []\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(split_index):  \n",
    "        X_tr = X_train.iloc[train_idx]\n",
    "        # type_weights = X_tr['Type'].value_counts(normalize=True).to_dict()\n",
    "        # sample_weight = (1/X_tr['Type'].apply(lambda x: type_weights[x])).values\n",
    "        X_val = X_train.iloc[valid_idx]\n",
    "        \n",
    "        y_tr = X_tr['AdoptionSpeed'].values    \n",
    "        y_val = X_val['AdoptionSpeed'].values\n",
    "            \n",
    "        d_train = xgb.DMatrix(X_tr[xgb_features], y_tr)\n",
    "        d_valid = xgb.DMatrix(X_val[xgb_features], y_val)\n",
    "        \n",
    "        since = time.time()\n",
    "        if print_result:\n",
    "            print('training XGB:')\n",
    "        # model = xgb.train(params, d_train, num_boost_round = 10000, evals=[(d_valid,'val')],\n",
    "        #                 early_stopping_rounds=100, \n",
    "        #                 verbose_eval=500)\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params, n_estimators=10000)\n",
    "        model.fit(\n",
    "        X=d_train.get_data(),\n",
    "        y=d_train.get_label(),  # Assuming d_train is a DMatrix object\n",
    "        eval_set=[(d_valid.get_data(), d_valid.get_label())],  # Assuming d_valid is a DMatrix object\n",
    "        # early_stopping_rounds=100,\n",
    "        # sample_weight=sample_weight,\n",
    "        verbose=500,\n",
    "    )\n",
    "        val_pred = model.predict(d_valid.get_data())\n",
    "        \n",
    "        oof_train_xgb[valid_idx] = val_pred\n",
    "        \n",
    "        hist = histogram(X_tr['AdoptionSpeed'].astype(int), \n",
    "                        int(np.min(X_train['AdoptionSpeed'])), \n",
    "                        int(np.max(X_train['AdoptionSpeed'])))\n",
    "        tr_cdf = get_cdf(hist)\n",
    "        \n",
    "        pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n",
    "        qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n",
    "        qwks.append(qwk)\n",
    "        if print_result:\n",
    "            print(\"QWK_2 = \", qwk, 'elapsed time:', time.time()-since)\n",
    "        \n",
    "    # print('overall rmse: %.5f'%rmse(oof_train_xgb, X_train['AdoptionSpeed']))\n",
    "    # print('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))\n",
    "    return np.mean(qwks), np.std(qwks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_pair =  {n: pd.DataFrame(col.unique(),columns=[n]) for n, col in data_df.items() if n in cat_cols}\n",
    "temp_dict = {}\n",
    "for i, (k, v) in enumerate(emd_pair.items()):\n",
    "    emb_vectors = pd.DataFrame(model.embeddings[0].weight.cpu().detach().numpy(), columns=[k +'_'+ str(i) for i in range(5)])\n",
    "    v = pd.concat([v, emb_vectors], axis=1)\n",
    "    temp_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "split_index = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_splits)\n",
    "for train_idx, valid_idx in kfold.split(data_df, data_df['AdoptionSpeed']):\n",
    "    split_index.append((train_idx, valid_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:1.17508\n",
      "[500]\tvalidation_0-rmse:1.03235\n",
      "[985]\tvalidation_0-rmse:1.02839\n",
      "[0]\tvalidation_0-rmse:1.17499\n",
      "[500]\tvalidation_0-rmse:1.01433\n",
      "[1000]\tvalidation_0-rmse:1.00851\n",
      "[1359]\tvalidation_0-rmse:1.00714\n",
      "[0]\tvalidation_0-rmse:1.17493\n",
      "[500]\tvalidation_0-rmse:1.02191\n",
      "[1000]\tvalidation_0-rmse:1.01253\n",
      "[1500]\tvalidation_0-rmse:1.01055\n",
      "[1726]\tvalidation_0-rmse:1.01036\n",
      "[0]\tvalidation_0-rmse:1.17507\n",
      "[500]\tvalidation_0-rmse:1.02460\n",
      "[1000]\tvalidation_0-rmse:1.01788\n",
      "[1300]\tvalidation_0-rmse:1.01646\n",
      "[0]\tvalidation_0-rmse:1.17495\n",
      "[500]\tvalidation_0-rmse:1.04501\n",
      "[1000]\tvalidation_0-rmse:1.04097\n",
      "[1485]\tvalidation_0-rmse:1.03985\n",
      "[0]\tvalidation_0-rmse:1.17491\n",
      "[500]\tvalidation_0-rmse:1.02434\n",
      "[1000]\tvalidation_0-rmse:1.01629\n",
      "[1349]\tvalidation_0-rmse:1.01503\n",
      "[0]\tvalidation_0-rmse:1.17562\n",
      "[500]\tvalidation_0-rmse:1.02302\n",
      "[1000]\tvalidation_0-rmse:1.01699\n",
      "[1175]\tvalidation_0-rmse:1.01668\n",
      "[0]\tvalidation_0-rmse:1.17534\n",
      "[500]\tvalidation_0-rmse:1.02818\n",
      "[1000]\tvalidation_0-rmse:1.01958\n",
      "[1500]\tvalidation_0-rmse:1.01415\n",
      "[1693]\tvalidation_0-rmse:1.01392\n",
      "[0]\tvalidation_0-rmse:1.17545\n",
      "[500]\tvalidation_0-rmse:1.02108\n",
      "[1000]\tvalidation_0-rmse:1.01290\n",
      "[1500]\tvalidation_0-rmse:1.01022\n",
      "[1509]\tvalidation_0-rmse:1.01014\n",
      "[0]\tvalidation_0-rmse:1.17551\n",
      "[500]\tvalidation_0-rmse:1.05090\n",
      "[856]\tvalidation_0-rmse:1.04680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.47323803440088785, 0.018399719969737147)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric':'rmse',\n",
    "        'tree_method':'hist',\n",
    "        'eta': 0.01,\n",
    "        'max_depth': 7,  \n",
    "        'subsample': 0.8,  \n",
    "        'colsample_bytree': 0.8,     \n",
    "        'alpha': 0.05,\n",
    "        'early_stopping_rounds': 100,\n",
    "}\n",
    "to_drop_columns = ['PetID', 'Name', 'RescuerID', 'AdoptionSpeed', 'Description',\n",
    "                    'BreedName_full','Breed1Name','Breed2Name']\n",
    "features = [x for x in data_df.columns if x not in to_drop_columns]\n",
    "xgb_features = features\n",
    "\n",
    "pet_xgboost(params, X_train=data_df, xgb_features= xgb_features, split_index=split_index, print_result=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Combining the embeddings with 5 dimontional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in temp_dict.items():\n",
    "    data_df = pd.merge(data_df, v, on=k, how='left')\n",
    "    \n",
    "new_features = [x for x in data_df.columns if x not in to_drop_columns]\n",
    "for k, v in temp_dict.items():\n",
    "    new_features.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:1.17539\n",
      "[500]\tvalidation_0-rmse:1.03430\n",
      "[1000]\tvalidation_0-rmse:1.02950\n",
      "[1202]\tvalidation_0-rmse:1.02930\n",
      "[0]\tvalidation_0-rmse:1.17550\n",
      "[500]\tvalidation_0-rmse:1.01586\n",
      "[1000]\tvalidation_0-rmse:1.00926\n",
      "[1500]\tvalidation_0-rmse:1.00618\n",
      "[1651]\tvalidation_0-rmse:1.00628\n",
      "[0]\tvalidation_0-rmse:1.17566\n",
      "[500]\tvalidation_0-rmse:1.02031\n",
      "[1000]\tvalidation_0-rmse:1.01267\n",
      "[1500]\tvalidation_0-rmse:1.01075\n",
      "[1641]\tvalidation_0-rmse:1.01144\n",
      "[0]\tvalidation_0-rmse:1.17518\n",
      "[500]\tvalidation_0-rmse:1.02710\n",
      "[1000]\tvalidation_0-rmse:1.02227\n",
      "[1044]\tvalidation_0-rmse:1.02225\n",
      "[0]\tvalidation_0-rmse:1.17532\n",
      "[500]\tvalidation_0-rmse:1.04442\n",
      "[1000]\tvalidation_0-rmse:1.03891\n",
      "[1230]\tvalidation_0-rmse:1.03876\n",
      "[0]\tvalidation_0-rmse:1.17519\n",
      "[500]\tvalidation_0-rmse:1.02697\n",
      "[1000]\tvalidation_0-rmse:1.02046\n",
      "[1333]\tvalidation_0-rmse:1.02008\n",
      "[0]\tvalidation_0-rmse:1.17576\n",
      "[500]\tvalidation_0-rmse:1.02417\n",
      "[1000]\tvalidation_0-rmse:1.01882\n",
      "[1278]\tvalidation_0-rmse:1.01875\n",
      "[0]\tvalidation_0-rmse:1.17580\n",
      "[500]\tvalidation_0-rmse:1.02663\n",
      "[1000]\tvalidation_0-rmse:1.01756\n",
      "[1422]\tvalidation_0-rmse:1.01567\n",
      "[0]\tvalidation_0-rmse:1.17584\n",
      "[500]\tvalidation_0-rmse:1.02077\n",
      "[1000]\tvalidation_0-rmse:1.01037\n",
      "[1500]\tvalidation_0-rmse:1.00666\n",
      "[1572]\tvalidation_0-rmse:1.00693\n",
      "[0]\tvalidation_0-rmse:1.17600\n",
      "[500]\tvalidation_0-rmse:1.05383\n",
      "[1000]\tvalidation_0-rmse:1.04821\n",
      "[1491]\tvalidation_0-rmse:1.04697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.46996193606590503, 0.021446443921121338)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pet_xgboost(params, X_train=data_df, xgb_features= new_features, split_index=split_index, print_result=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Combining the embeddings with 10 dimontional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network_setting import *\n",
    "emb_szs = [\n",
    "    (c, min(10, (c + 1) // 2)) for _, c in emb_c.items()\n",
    "]  # embedding sizes for the chosen columns\n",
    "\n",
    "model = PetFinderModel(emb_szs, n_cont)\n",
    "model.load_state_dict(torch.load(path + \"/model-stratify-10.pt\"))\n",
    "# model.eval()\n",
    "emd_pair =  {n: pd.DataFrame(col.unique(),columns=[n]) for n, col in data_df.items() if n in cat_cols}\n",
    "temp_dict = {}\n",
    "for i, (k, v) in enumerate(emd_pair.items()):\n",
    "    emb_vectors = pd.DataFrame(model.embeddings[0].weight.cpu().detach().numpy(), columns=[k +'_'+ str(i) for i in range(10)])\n",
    "    v = pd.concat([v, emb_vectors], axis=1)\n",
    "    temp_dict[k] = v\n",
    "    \n",
    "for k, v in temp_dict.items():\n",
    "    data_df = pd.merge(data_df, v, on=k, how='left')\n",
    "    \n",
    "new_features_10 = [x for x in data_df.columns if x not in to_drop_columns]\n",
    "for k, v in temp_dict.items():\n",
    "    new_features_10.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:1.17530\n",
      "[500]\tvalidation_0-rmse:1.03140\n",
      "[1000]\tvalidation_0-rmse:1.02687\n",
      "[1247]\tvalidation_0-rmse:1.02652\n",
      "[0]\tvalidation_0-rmse:1.17516\n",
      "[500]\tvalidation_0-rmse:1.01578\n",
      "[1000]\tvalidation_0-rmse:1.00948\n",
      "[1455]\tvalidation_0-rmse:1.00815\n",
      "[0]\tvalidation_0-rmse:1.17493\n",
      "[500]\tvalidation_0-rmse:1.02111\n",
      "[1000]\tvalidation_0-rmse:1.01302\n",
      "[1311]\tvalidation_0-rmse:1.01176\n",
      "[0]\tvalidation_0-rmse:1.17495\n",
      "[500]\tvalidation_0-rmse:1.02490\n",
      "[1000]\tvalidation_0-rmse:1.01639\n",
      "[1292]\tvalidation_0-rmse:1.01454\n",
      "[0]\tvalidation_0-rmse:1.17501\n",
      "[500]\tvalidation_0-rmse:1.04494\n",
      "[871]\tvalidation_0-rmse:1.04230\n",
      "[0]\tvalidation_0-rmse:1.17501\n",
      "[500]\tvalidation_0-rmse:1.02921\n",
      "[1000]\tvalidation_0-rmse:1.02228\n",
      "[1386]\tvalidation_0-rmse:1.02048\n",
      "[0]\tvalidation_0-rmse:1.17533\n",
      "[500]\tvalidation_0-rmse:1.02457\n",
      "[981]\tvalidation_0-rmse:1.01950\n",
      "[0]\tvalidation_0-rmse:1.17527\n",
      "[500]\tvalidation_0-rmse:1.02900\n",
      "[1000]\tvalidation_0-rmse:1.02278\n",
      "[1500]\tvalidation_0-rmse:1.01807\n",
      "[1649]\tvalidation_0-rmse:1.01810\n",
      "[0]\tvalidation_0-rmse:1.17530\n",
      "[500]\tvalidation_0-rmse:1.02141\n",
      "[1000]\tvalidation_0-rmse:1.01136\n",
      "[1371]\tvalidation_0-rmse:1.00847\n",
      "[0]\tvalidation_0-rmse:1.17578\n",
      "[500]\tvalidation_0-rmse:1.05249\n",
      "[1000]\tvalidation_0-rmse:1.04839\n",
      "[1308]\tvalidation_0-rmse:1.04763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4753089818891361, 0.01566751316252965)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pet_xgboost(params, X_train=data_df, xgb_features= new_features_10, split_index=split_index, print_result=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
